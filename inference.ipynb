{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with LoRA Fine-tuned Qwen Model\n",
    "\n",
    "This notebook demonstrates how to use your fine-tuned model for:\n",
    "- Translation (EN ‚Üî RU)\n",
    "- Question Answering (EN & RU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shara/usi-nlp-EagleSFT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"Qwen/Qwen2.5-0.5B\"\n",
    "LORA_ROOT = \"./qwen2.5_lora\"   # folder with checkpoints or final model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using final model: ./qwen2.5_lora/final\n"
     ]
    }
   ],
   "source": [
    "def get_model_path(lora_root):\n",
    "    \"\"\"Get the path to the LoRA model (final > checkpoint > epoch)\"\"\"\n",
    "    # Check for final model\n",
    "    final_path = os.path.join(lora_root, \"final\")\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"‚úÖ Using final model: {final_path}\")\n",
    "        return final_path\n",
    "    \n",
    "    # Check for checkpoints\n",
    "    checkpoints = [\n",
    "        d for d in os.listdir(lora_root)\n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(lora_root, d))\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "        checkpoint_path = os.path.join(lora_root, checkpoints[-1])\n",
    "        print(f\"‚úÖ Using latest checkpoint: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    # Check for epoch folders\n",
    "    epochs = [\n",
    "        d for d in os.listdir(lora_root)\n",
    "        if d.startswith(\"epoch_\") and os.path.isdir(os.path.join(lora_root, d))\n",
    "    ]\n",
    "    if epochs:\n",
    "        epochs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "        epoch_path = os.path.join(lora_root, epochs[-1])\n",
    "        print(f\"‚úÖ Using latest epoch: {epoch_path}\")\n",
    "        return epoch_path\n",
    "    \n",
    "    raise RuntimeError(f\"No model found in {lora_root}\")\n",
    "\n",
    "LORA_PATH = get_model_path(LORA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading base model...\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Loading LoRA adapters...\n",
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"üß© Loading LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_PATH,\n",
    "    torch_dtype=DTYPE\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_new_tokens=512, temperature=0.0, top_p=1.0):\n",
    "    \"\"\"\n",
    "    Generate a response from the model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user's question or instruction\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0 = greedy/deterministic)\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    # Format the message\n",
    "    message = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        message,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature if temperature > 0 else None,\n",
    "            top_p=top_p if temperature > 0 else None,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    output_text = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nResponse: {output_text}\")\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Translation (Russian ‚Üí English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Translate the following question to English:\n",
      "\n",
      "–ö–æ–≥–¥–∞ –±—ã–ª–∞ –≤–æ–π–Ω–∞?\n",
      "\n",
      "Response: When was the war?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Translate the following question to English:\\n\\n–ö–æ–≥–¥–∞ –±—ã–ª–∞ –≤–æ–π–Ω–∞?\"\n",
    "\n",
    "response = generate_response(PROMPT, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Translation (English ‚Üí Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Translate the following question to Russian:\n",
      "\n",
      "What is artificial intelligence?\n",
      "\n",
      "Response: –ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Translate the following question to Russian:\\n\\nWhat is artificial intelligence?\"\n",
    "\n",
    "response = generate_response(PROMPT, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Question Answering (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is machine learning?\n",
      "\n",
      "Response: Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to learn and improve from experience. It involves the use of statistical techniques to enable computers to automatically learn and improve from data, without being explicitly programmed to perform a specific task.\n",
      "\n",
      "Here are some key aspects of machine learning:\n",
      "\n",
      "1. **Data Collection**: Machine learning requires data to train models. This data can be in the form of text, images, audio, or any other type of data that can be processed by the algorithm.\n",
      "\n",
      "2. **Model Training**: The algorithm learns from the data to identify patterns and make predictions or decisions. This is typically done using supervised learning, where the algorithm is given labeled data and learns to predict the output based on the input.\n",
      "\n",
      "3. **Evaluation**: The performance of the model is evaluated using metrics such as accuracy, precision, recall, and F1 score. This helps in understanding how well the model is performing.\n",
      "\n",
      "4. **Deployment**: Once the model is trained and evaluated, it can be deployed to make predictions or decisions on new, unseen data.\n",
      "\n",
      "5. **Adaptation**: Machine learning models can be adapted to new data to improve their performance over time. This is often done through techniques like transfer learning, where a pre-trained\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"What is machine learning?\"\n",
    "\n",
    "response = generate_response(PROMPT, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Question Answering (Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: –ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?\n",
      "\n",
      "Response: –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (–º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è) –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–µ—à–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è, —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\n",
      "\n",
      "–û—Å–Ω–æ–≤–Ω—ã–µ —ç—Ç–∞–ø—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:\n",
      "\n",
      "1. **–ò–∑—É—á–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å, –∏ –∞–Ω–∞–ª–∏–∑ –µ–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.\n",
      "2. **–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤**: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ª–∏–Ω–µ–π–Ω—ã–π —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω, –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, —Å–ª—É—á–∞–π–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è –∏ –¥—Ä—É–≥–∏–µ.\n",
      "3. **–û–±—É—á–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤**: –û–±—É—á–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.\n",
      "4. **–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞**: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?\"\n",
    "\n",
    "response = generate_response(PROMPT, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Inferences\n",
    "\n",
    "Test multiple prompts at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test 1/4\n",
      "Prompt: Translate the following question to Russian:\n",
      "\n",
      "How does photosynthesis work?\n",
      "\n",
      "Response: –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Photosynthesis?\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 2/4\n",
      "Prompt: Translate the following question to English:\n",
      "\n",
      "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑?\n",
      "\n",
      "Response: How does photo synthesis work?\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 3/4\n",
      "Prompt: What is the capital of Russia?\n",
      "\n",
      "Response: The capital of Russia is Moscow. It is the largest city in Russia and the country's political, economic, and cultural center. Moscow is located in the northwestern part of the country and is the capital of the Russian Federation.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 4/4\n",
      "Prompt: –ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏?\n",
      "\n",
      "Response: –†–æ—Å—Å–∏—è –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–ª–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∫–∏, –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –Ω–∞—Å–ª–µ–¥–∏—è —Å—Ç—Ä–∞–Ω—ã. –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö:\n",
      "\n",
      "1. **–ú–æ—Å–∫–≤–∞** ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –∏ —Ü–µ–Ω—Ç—Ä —ç–∫–æ–Ω–æ–º–∏–∫–∏, –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –∫—É–ª—å—Ç—É—Ä—ã. –û–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–º –≥–æ—Ä–æ–¥–æ–º –∏ —Ü–µ–Ω—Ç—Ä–æ–º –≥–æ—Ä–æ–¥–∞, –∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –º–µ—Å—Ç–æ–º –¥–ª—è –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –Ω–∞—Å–ª–µ–¥–∏—è, –≤–∫–ª—é—á–∞—è –º—É–∑–µ–∏, —Ç–µ–∞—Ç—Ä—ã –∏ –¥—Ä—É–≥–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã.\n",
      "\n",
      "2. **–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥** ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –∏ —Ü–µ–Ω—Ç—Ä —ç–∫–æ–Ω–æ–º–∏–∫–∏, –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –∫—É–ª—å—Ç—É—Ä—ã. –û–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –≥–æ—Ä–æ–¥–æ–º –∏ —Ü–µ–Ω—Ç—Ä–æ–º –≥–æ—Ä–æ–¥–∞, –∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –º–µ—Å—Ç–æ–º –¥–ª—è –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –Ω–∞—Å–ª–µ–¥–∏—è, –≤–∫–ª—é—á–∞—è –º—É–∑–µ–∏, —Ç–µ–∞—Ç—Ä—ã –∏ –¥—Ä—É–≥–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã.\n",
      "\n",
      "3. **–ö–∞–∑–∞–Ω—å** ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –∏ —Ü–µ–Ω—Ç—Ä —ç–∫–æ–Ω–æ–º–∏–∫–∏, –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –∫—É–ª—å—Ç—É—Ä—ã. –û–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –≥–æ—Ä–æ–¥–æ–º –∏ —Ü–µ–Ω—Ç—Ä–æ–º –≥–æ—Ä–æ–¥–∞, –∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –º–µ—Å—Ç–æ–º –¥–ª—è –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –Ω–∞—Å–ª–µ–¥–∏—è, –≤–∫–ª—é—á–∞—è –º—É–∑–µ–∏, —Ç–µ–∞—Ç—Ä—ã –∏ –¥—Ä—É–≥–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã.\n",
      "\n",
      "4. **–°–æ—á–∏** ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –∏ —Ü–µ–Ω—Ç—Ä —ç–∫–æ–Ω–æ–º–∏–∫–∏, –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –∫—É–ª—å—Ç—É—Ä—ã. –û–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è\n"
     ]
    }
   ],
   "source": [
    "test_prompts = [\n",
    "    \"Translate the following question to Russian:\\n\\nHow does photosynthesis work?\",\n",
    "    \"Translate the following question to English:\\n\\n–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑?\",\n",
    "    \"What is the capital of Russia?\",\n",
    "    \"–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏?\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i}/{len(test_prompts)}\")\n",
    "    response = generate_response(prompt, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
